#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ğ’ĞµÑ€ÑĞ¸Ñ analyze_data.py Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Telegram Ğ±Ğ¾Ñ‚Ğµ
Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ²Ğ¸Ğ´Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² ĞºĞ¾Ğ½ÑĞ¾Ğ»ÑŒ
"""

import subprocess
import os
import time
import sys
import io
import locale
import warnings

# ĞŸĞ¾Ğ´Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ñ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾ Python Ğ²ĞµÑ€ÑĞ¸ÑÑ…
warnings.filterwarnings('ignore', category=DeprecationWarning)

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸
try:
    locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')
except:
    try:
        locale.setlocale(locale.LC_ALL, 'C.UTF-8')
    except:
        pass

os.environ['PYTHONIOENCODING'] = 'utf-8'

try:
    if hasattr(sys.stdout, 'buffer'):
        if sys.stdout.encoding != 'utf-8':
            sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    if hasattr(sys.stderr, 'buffer'):
        if sys.stderr.encoding != 'utf-8':
            sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
except:
    pass

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, avg, min as spark_min, max as spark_max, 
    count, sum as spark_sum, stddev, round as spark_round
)
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType

def create_spark_session():
    """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Spark ÑĞµÑÑĞ¸Ğ¸"""
    spark = SparkSession.builder \
        .appName("ComponentsPriceAnalysis") \
        .master("spark://spark-master:7077") \
        .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
        .getOrCreate()
    return spark

def load_data(spark, hdfs_path):
    """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· HDFS"""
    schema = StructType([
        StructField("Component_Type", StringType(), True),
        StructField("Manufacturer", StringType(), True),
        StructField("Model", StringType(), True),
        StructField("Spec_1", StringType(), True),
        StructField("Spec_2", StringType(), True),
        StructField("Spec_3", StringType(), True),
        StructField("Year", IntegerType(), True),
        StructField("Month", IntegerType(), True),
        StructField("Day", IntegerType(), True),
        StructField("Week", IntegerType(), True),
        StructField("Merchant", StringType(), True),
        StructField("Region_Code", StringType(), True),
        StructField("Currency", StringType(), True),
        StructField("Price_USD", DoubleType(), True),
        StructField("Price_Original", DoubleType(), True)
    ])
    
    df = spark.read \
        .option("header", "true") \
        .schema(schema) \
        .csv(hdfs_path)
    
    return df

def get_component_metrics(df_component, component_type):
    """ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°"""
    output_lines = []
    
    # Ğ­Ğ¼Ğ¾Ğ´Ğ·Ğ¸ Ğ´Ğ»Ñ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²
    emoji_map = {
        'CPU': 'ğŸ–¥ï¸',
        'GPU': 'ğŸ®',
        'RAM': 'ğŸ’¾'
    }
    emoji = emoji_map.get(component_type, 'ğŸ”§')
    
    output_lines.append("{} *{} - Ğ”Ğ•Ğ¢ĞĞ›Ğ¬ĞĞĞ¯ Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ*".format(emoji, component_type))
    output_lines.append("")
    
    # ĞĞ±Ñ‰Ğ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ¿Ğ¾ Ñ‚Ğ¸Ğ¿Ñƒ
    total_records = df_component.count()
    output_lines.append("ğŸ“ ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹: *{:,}*".format(total_records))
    output_lines.append("")
    
    # Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ¿Ğ¾ Ñ†ĞµĞ½Ğ°Ğ¼
    output_lines.append("ğŸ’° *Ğ¦ĞµĞ½Ñ‹ (USD)*")
    price_stats = df_component.agg(
        spark_round(avg("Price_USD"), 2).alias("Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ Ñ†ĞµĞ½Ğ°"),
        spark_round(spark_min("Price_USD"), 2).alias("ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ†ĞµĞ½Ğ°"),
        spark_round(spark_max("Price_USD"), 2).alias("ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ†ĞµĞ½Ğ°"),
        spark_round(stddev("Price_USD"), 2).alias("Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ğµ")
    ).collect()[0]
    
    output_lines.append("ğŸ“Š Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ: *${}*".format(price_stats['Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ Ñ†ĞµĞ½Ğ°']))
    output_lines.append("ğŸ“‰ ĞœĞ¸Ğ½: *${}*".format(price_stats['ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ†ĞµĞ½Ğ°']))
    output_lines.append("ğŸ“ˆ ĞœĞ°ĞºÑ: *${}*".format(price_stats['ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ†ĞµĞ½Ğ°']))
    output_lines.append("ğŸ“ ĞÑ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ğµ: *${}*".format(price_stats['Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ğµ']))
    output_lines.append("")
    
    # Ğ¢Ğ¾Ğ¿-5 Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ°
    output_lines.append("ğŸ­ *Ğ¢Ğ¾Ğ¿-5 Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ĞµĞ¹*")
    manufacturer_stats = df_component.groupBy("Manufacturer").agg(
        count("*").alias("ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹"),
        spark_round(avg("Price_USD"), 2).alias("Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ Ñ†ĞµĞ½Ğ°")
    ).orderBy(col("ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹").desc())
    
    rows = manufacturer_stats.limit(5).collect()
    if rows:
        for i, row in enumerate(rows, 1):
            output_lines.append("{}. *{}*".format(i, row['Manufacturer']))
            output_lines.append("   â”” {:,} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ | Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ: *${}*".format(
                row['ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹'],
                row['Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ Ñ†ĞµĞ½Ğ°']
            ))
    else:
        output_lines.append("   ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")
    output_lines.append("")
    
    # Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ¿Ğ¾ Ğ³Ğ¾Ğ´Ğ°Ğ¼
    output_lines.append("ğŸ“… *ĞŸĞ¾ Ğ³Ğ¾Ğ´Ğ°Ğ¼*")
    year_stats = df_component.groupBy("Year").agg(
        count("*").alias("ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹"),
        spark_round(avg("Price_USD"), 2).alias("Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ Ñ†ĞµĞ½Ğ°")
    ).orderBy("Year")
    
    rows = year_stats.collect()
    if rows:
        for row in rows:
            output_lines.append("â€¢ *{}*: {:,} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ | Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ: *${}*".format(
                row['Year'],
                row['ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹'],
                row['Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ Ñ†ĞµĞ½Ğ°']
            ))
    else:
        output_lines.append("   ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")
    output_lines.append("")
    
    # Ğ¢Ğ¾Ğ¿-5 Ğ¼ĞµÑ€Ñ‡Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ°
    output_lines.append("ğŸ›’ *Ğ¢Ğ¾Ğ¿-5 Ğ¼ĞµÑ€Ñ‡Ğ°Ğ½Ñ‚Ğ¾Ğ²*")
    merchant_stats = df_component.groupBy("Merchant").agg(
        count("*").alias("ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹"),
        spark_round(avg("Price_USD"), 2).alias("Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ Ñ†ĞµĞ½Ğ°")
    ).orderBy(col("ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹").desc())
    
    rows = merchant_stats.limit(5).collect()
    if rows:
        for i, row in enumerate(rows, 1):
            output_lines.append("{}. *{}*".format(i, row['Merchant']))
            output_lines.append("   â”” {:,} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ | Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ: *${}*".format(
                row['ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹'],
                row['Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ Ñ†ĞµĞ½Ğ°']
            ))
    else:
        output_lines.append("   ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")
    output_lines.append("")
    
    # ĞšĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ°
    try:
        correlation = df_component.stat.corr("Year", "Price_USD")
        if correlation is not None:
            corr_emoji = "ğŸ“ˆ" if correlation > 0 else "ğŸ“‰" if correlation < 0 else "â¡ï¸"
            output_lines.append("ğŸ”— *ĞšĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ³Ğ¾Ğ´-Ñ†ĞµĞ½Ğ°*")
            output_lines.append("{} *{:.4f}*".format(corr_emoji, correlation))
            if abs(correlation) < 0.1:
                output_lines.append("   (Ğ¾Ñ‡ĞµĞ½ÑŒ ÑĞ»Ğ°Ğ±Ğ°Ñ ÑĞ²ÑĞ·ÑŒ)")
            elif abs(correlation) < 0.3:
                output_lines.append("   (ÑĞ»Ğ°Ğ±Ğ°Ñ ÑĞ²ÑĞ·ÑŒ)")
            elif abs(correlation) < 0.5:
                output_lines.append("   (ÑƒĞ¼ĞµÑ€ĞµĞ½Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ)")
            else:
                output_lines.append("   (ÑĞ¸Ğ»ÑŒĞ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ)")
            output_lines.append("")
    except:
        pass
    
    return "\n".join(output_lines)


def get_metrics_string(df):
    """ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ² Ğ²Ğ¸Ğ´Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ¸ Ñ ĞºÑ€Ğ°ÑĞ¸Ğ²Ñ‹Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼"""
    output_lines = []
    
    output_lines.append("ğŸ“Š *ĞšĞ›Ğ®Ğ§Ğ•Ğ’Ğ«Ğ• ĞœĞ•Ğ¢Ğ Ğ˜ĞšĞ˜ Ğ”ĞĞ¢ĞĞ¡Ğ•Ğ¢Ğ ĞšĞĞœĞŸĞĞĞ•ĞĞ¢ĞĞ’*")
    output_lines.append("")
    
    # ĞĞ±Ñ‰Ğ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°
    output_lines.append("ğŸ“ˆ *ĞĞ‘Ğ©ĞĞ¯ Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ*")
    total_records = df.count()
    output_lines.append("ğŸ“ Ğ’ÑĞµĞ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹: *{:,}*".format(total_records))
    output_lines.append("")
    
    # ĞšÑ€Ğ°Ñ‚ĞºĞ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ¿Ğ¾ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²
    output_lines.append("ğŸ”§ *Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼*")
    component_stats = df.groupBy("Component_Type").agg(
        count("*").alias("ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾"),
        spark_round(avg("Price_USD"), 2).alias("Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ Ñ†ĞµĞ½Ğ°"),
        spark_round(spark_min("Price_USD"), 2).alias("ĞœĞ¸Ğ½. Ñ†ĞµĞ½Ğ°"),
        spark_round(spark_max("Price_USD"), 2).alias("ĞœĞ°ĞºÑ. Ñ†ĞµĞ½Ğ°")
    ).orderBy("Component_Type")
    
    rows = component_stats.collect()
    for row in rows:
        output_lines.append("â€¢ *{}*: {:,} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ | Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ: *${}*".format(
            row['Component_Type'],
            row['ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾'],
            row['Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ Ñ†ĞµĞ½Ğ°']
        ))
    output_lines.append("")
    output_lines.append("â”€" * 40)
    output_lines.append("")
    
    # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ²ÑĞµÑ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²
    component_types = [row['Component_Type'] for row in rows]
    
    # Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ° Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ğ¼ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ
    for comp_type in component_types:
        df_component = df.filter(col("Component_Type") == comp_type)
        component_metrics = get_component_metrics(df_component, comp_type)
        output_lines.append(component_metrics)
        output_lines.append("â”€" * 40)
        output_lines.append("")
    
    output_lines.append("âœ… *ĞĞĞĞ›Ğ˜Ğ— Ğ—ĞĞ’Ğ•Ğ Ğ¨Ğ•Ğ*")
    
    return "\n".join(output_lines)

def main():
    """Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ"""
    # ĞŸĞ¾Ğ´Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ñ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Spark
    import logging
    logging.getLogger("pyspark").setLevel(logging.ERROR)
    logging.getLogger("py4j").setLevel(logging.ERROR)
    
    hdfs_path = "hdfs://namenode:9000/data/all_components_prices.csv"
    local_file = "all_components_prices.csv"
    
    spark = create_spark_session()
    
    # ĞĞ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼ Spark Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹
    spark.sparkContext.setLogLevel("ERROR")
    
    try:
        # ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ¸Ğ· HDFS
        try:
            df = load_data(spark, hdfs_path)
        except:
            # Ğ•ÑĞ»Ğ¸ Ğ½Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ»Ğ¾ÑÑŒ, Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¸Ğ· Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ğ°
            local_paths = [
                local_file,
                "/data/{}".format(local_file),
                "/opt/spark/work-dir/{}".format(local_file)
            ]
            for path in local_paths:
                if os.path.exists(path):
                    df = load_data(spark, "file://{}".format(path))
                    break
            else:
                raise Exception("Ğ¤Ğ°Ğ¹Ğ» Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½")
        
        df.cache()
        results = get_metrics_string(df)
        print(results)
        
    except Exception as e:
        print("ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°: {}".format(str(e)))
        import traceback
        traceback.print_exc()
    
    finally:
        spark.stop()

if __name__ == "__main__":
    main()

